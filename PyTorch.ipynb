{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch as torch\n",
    "from keras.datasets import fashion_mnist\n",
    "import torch\n",
    "(xtrain, ytrain), (xtest, ytest) = fashion_mnist.load_data()\n",
    "xtrain = xtrain.reshape(60000, 784)\n",
    "ytrain, ytest = ytrain.copy(), ytest.copy()\n",
    "ytrain[ytrain != 8] = 0\n",
    "ytrain[ytrain == 8] = 1\n",
    "ytest[ytest != 8] = 0\n",
    "ytest[ytest == 8] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor(2., requires_grad = True)\n",
    "b = torch.tensor(3.)\n",
    "c = a*b\n",
    "c.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones(10, requires_grad = True)\n",
    "sig = torch.nn.Sigmoid()\n",
    "b = sig(a).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1966, 0.1966, 0.1966, 0.1966, 0.1966, 0.1966, 0.1966, 0.1966, 0.1966,\n",
       "        0.1966])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Feedforward, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.fc1 = torch.nn.Linear(self.input_size, self.hidden_size)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtensor = torch.transpose(torch.from_numpy(xtrain), 0, 1).float()\n",
    "ytensor = torch.from_numpy(ytrain).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-153-f27f8c0c0213>:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  W = torch.tensor(initval, requires_grad = True)\n"
     ]
    }
   ],
   "source": [
    "# W = 0.01 * torch.rand((1, 784), requires_grad = True) - 0.005\n",
    "W = torch.tensor(initval, requires_grad = True)\n",
    "H = torch.mv(W, xtensor[:,0])\n",
    "sig = torch.nn.Sigmoid()\n",
    "output = sig(H)\n",
    "loss = ytensor[0] * torch.log(output) + (1 - ytensor[0]) * torch.log(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           1.,   0.,   0.,  13.,  73.,   0.,   0.,   1.,   4.,   0.,   0.,   0.,\n",
       "           0.,   1.,   1.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.,   3.,   0.,  36., 136., 127.,  62.,  54.,   0.,\n",
       "           0.,   0.,   1.,   3.,   4.,   0.,   0.,   3.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   6.,   0., 102., 204.,\n",
       "         176., 134., 144., 123.,  23.,   0.,   0.,   0.,   0.,  12.,  10.,   0.,\n",
       "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0., 155., 236., 207., 178., 107., 156., 161., 109.,  64.,  23.,\n",
       "          77., 130.,  72.,  15.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   1.,   0.,  69., 207., 223., 218., 216., 216., 163.,\n",
       "         127., 121., 122., 146., 141.,  88., 172.,  66.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.,   0.,   1.,   1.,   1.,   0., 200., 232., 232.,\n",
       "         233., 229., 223., 223., 215., 213., 164., 127., 123., 196., 229.,   0.,\n",
       "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0., 183., 225., 216., 223., 228., 235., 227., 224., 222., 224., 221.,\n",
       "         223., 245., 173.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.,   0., 193., 228., 218., 213., 198., 180., 212.,\n",
       "         210., 211., 213., 223., 220., 243., 202.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.,   0.,   1.,   3.,   0.,  12., 219., 220., 212.,\n",
       "         218., 192., 169., 227., 208., 218., 224., 212., 226., 197., 209.,  52.,\n",
       "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   6.,   0.,\n",
       "          99., 244., 222., 220., 218., 203., 198., 221., 215., 213., 222., 220.,\n",
       "         245., 119., 167.,  56.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   4.,   0.,   0.,  55., 236., 228., 230., 228., 240., 232., 213.,\n",
       "         218., 223., 234., 217., 217., 209.,  92.,   0.,   0.,   0.,   1.,   4.,\n",
       "           6.,   7.,   2.,   0.,   0.,   0.,   0.,   0., 237., 226., 217., 223.,\n",
       "         222., 219., 222., 221., 216., 223., 229., 215., 218., 255.,  77.,   0.,\n",
       "           0.,   3.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  62., 145., 204.,\n",
       "         228., 207., 213., 221., 218., 208., 211., 218., 224., 223., 219., 215.,\n",
       "         224., 244., 159.,   0.,   0.,   0.,   0.,   0.,  18.,  44.,  82., 107.,\n",
       "         189., 228., 220., 222., 217., 226., 200., 205., 211., 230., 224., 234.,\n",
       "         176., 188., 250., 248., 233., 238., 215.,   0.,   0.,  57., 187., 208.,\n",
       "         224., 221., 224., 208., 204., 214., 208., 209., 200., 159., 245., 193.,\n",
       "         206., 223., 255., 255., 221., 234., 221., 211., 220., 232., 246.,   0.,\n",
       "           3., 202., 228., 224., 221., 211., 211., 214., 205., 205., 205., 220.,\n",
       "         240.,  80., 150., 255., 229., 221., 188., 154., 191., 210., 204., 209.,\n",
       "         222., 228., 225.,   0.,  98., 233., 198., 210., 222., 229., 229., 234.,\n",
       "         249., 220., 194., 215., 217., 241.,  65.,  73., 106., 117., 168., 219.,\n",
       "         221., 215., 217., 223., 223., 224., 229.,  29.,  75., 204., 212., 204.,\n",
       "         193., 205., 211., 225., 216., 185., 197., 206., 198., 213., 240., 195.,\n",
       "         227., 245., 239., 223., 218., 212., 209., 222., 220., 221., 230.,  67.,\n",
       "          48., 203., 183., 194., 213., 197., 185., 190., 194., 192., 202., 214.,\n",
       "         219., 221., 220., 236., 225., 216., 199., 206., 186., 181., 177., 172.,\n",
       "         181., 205., 206., 115.,   0., 122., 219., 193., 179., 171., 183., 196.,\n",
       "         204., 210., 213., 207., 211., 210., 200., 196., 194., 191., 195., 191.,\n",
       "         198., 192., 176., 156., 167., 177., 210.,  92.,   0.,   0.,  74., 189.,\n",
       "         212., 191., 175., 172., 175., 181., 185., 188., 189., 188., 193., 198.,\n",
       "         204., 209., 210., 210., 211., 188., 188., 194., 192., 216., 170.,   0.,\n",
       "           2.,   0.,   0.,   0.,  66., 200., 222., 237., 239., 242., 246., 243.,\n",
       "         244., 221., 220., 193., 191., 179., 182., 182., 181., 176., 166., 168.,\n",
       "          99.,  58.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  40.,\n",
       "          61.,  44.,  72.,  41.,  35.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           0.,   0.,   0.,   0.]])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "initval = torch.from_numpy(np.random.normal(0, 0.01, (1, 784))).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-138-b50b072300e8>:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  W = torch.tensor(initval, requires_grad = True)\n"
     ]
    }
   ],
   "source": [
    "W = torch.tensor(initval, requires_grad = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "initval = torch.from_numpy(np.random.normal(0, 0.01, (1, 784))).float()\n",
    "W = torch.zeros((1, 784), requires_grad = True)\n",
    "W = torch.nn.init.xavier_uniform_(W, gain = 0.0011)\n",
    "for j in range(10):\n",
    "    for i in range(10000):\n",
    "        H = torch.mv(W, xtensor[:,i])\n",
    "        sig = torch.nn.Sigmoid()\n",
    "        output = sig(H)\n",
    "        loss = -ytensor[i] * torch.log(output) - (1 - ytensor[i]) * torch.log(output)\n",
    "        loss.backward()\n",
    "#         losses.append()\n",
    "        with torch.no_grad():\n",
    "            W -= 0.000001 * W.grad\n",
    "            W.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.], grad_fn=<SubBackward0>)\n",
      "tensor([-0.], grad_fn=<SubBackward0>)\n",
      "tensor([4.7684e-07], grad_fn=<SubBackward0>)\n",
      "tensor([-0.], grad_fn=<SubBackward0>)\n",
      "tensor([-0.], grad_fn=<SubBackward0>)\n",
      "tensor([-0.], grad_fn=<SubBackward0>)\n",
      "tensor([-0.], grad_fn=<SubBackward0>)\n",
      "tensor([-0.], grad_fn=<SubBackward0>)\n",
      "tensor([0.0001], grad_fn=<SubBackward0>)\n",
      "tensor([-0.], grad_fn=<SubBackward0>)\n",
      "tensor([-0.], grad_fn=<SubBackward0>)\n",
      "tensor([-0.], grad_fn=<SubBackward0>)\n",
      "tensor([-0.], grad_fn=<SubBackward0>)\n",
      "tensor([7.1526e-07], grad_fn=<SubBackward0>)\n",
      "tensor([1.5497e-06], grad_fn=<SubBackward0>)\n",
      "tensor([-0.], grad_fn=<SubBackward0>)\n",
      "tensor([-0.], grad_fn=<SubBackward0>)\n",
      "tensor([-0.], grad_fn=<SubBackward0>)\n",
      "tensor([-0.], grad_fn=<SubBackward0>)\n",
      "tensor([3.5763e-07], grad_fn=<SubBackward0>)\n",
      "tensor([-0.], grad_fn=<SubBackward0>)\n",
      "tensor([-0.], grad_fn=<SubBackward0>)\n",
      "tensor([-0.], grad_fn=<SubBackward0>)\n",
      "tensor([-0.], grad_fn=<SubBackward0>)\n",
      "tensor([-0.], grad_fn=<SubBackward0>)\n",
      "tensor([-0.], grad_fn=<SubBackward0>)\n",
      "tensor([-0.], grad_fn=<SubBackward0>)\n",
      "tensor([-0.], grad_fn=<SubBackward0>)\n",
      "tensor([-0.], grad_fn=<SubBackward0>)\n",
      "tensor([-0.], grad_fn=<SubBackward0>)\n",
      "tensor([0.0002], grad_fn=<SubBackward0>)\n",
      "tensor([-0.], grad_fn=<SubBackward0>)\n",
      "tensor([-0.], grad_fn=<SubBackward0>)\n",
      "tensor([0.0007], grad_fn=<SubBackward0>)\n",
      "tensor([5.2573e-05], grad_fn=<SubBackward0>)\n",
      "tensor([-0.], grad_fn=<SubBackward0>)\n",
      "tensor([-0.], grad_fn=<SubBackward0>)\n",
      "tensor([-0.], grad_fn=<SubBackward0>)\n",
      "tensor([-0.], grad_fn=<SubBackward0>)\n",
      "tensor([-0.], grad_fn=<SubBackward0>)\n",
      "tensor([-0.], grad_fn=<SubBackward0>)\n",
      "tensor([-0.], grad_fn=<SubBackward0>)\n",
      "tensor([-0.], grad_fn=<SubBackward0>)\n",
      "tensor([-0.], grad_fn=<SubBackward0>)\n",
      "tensor([-0.], grad_fn=<SubBackward0>)\n",
      "tensor([-0.], grad_fn=<SubBackward0>)\n",
      "tensor([-0.], grad_fn=<SubBackward0>)\n",
      "tensor([-0.], grad_fn=<SubBackward0>)\n",
      "tensor([-0.], grad_fn=<SubBackward0>)\n",
      "tensor([-0.], grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for i in range(50):\n",
    "    H = torch.mv(W, xtensor[:,i])\n",
    "    sig = torch.nn.Sigmoid()\n",
    "    output = sig(H)\n",
    "    loss = -ytensor[i] * torch.log(output) - (1 - ytensor[i]) * torch.log(output)\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.0363e-05,  8.8873e-05,  3.5210e-05, -1.4706e-05,  8.6917e-05,\n",
       "         -5.2186e-05,  7.3739e-05,  9.5012e-05, -4.9663e-05, -1.6828e-04,\n",
       "         -4.3513e-04, -6.4321e-04, -3.7131e-04, -2.9355e-04, -2.2788e-04,\n",
       "         -3.7441e-04, -4.9457e-04, -6.8395e-04, -4.1283e-04, -4.2822e-05,\n",
       "         -5.6733e-05,  7.3563e-05,  4.5582e-05, -2.0960e-05, -8.8014e-05,\n",
       "         -6.8810e-05, -3.4419e-05, -7.6996e-05,  8.9316e-05,  5.9910e-05,\n",
       "          5.9616e-05,  1.7217e-06,  3.2591e-05,  7.0256e-05, -2.1169e-05,\n",
       "         -4.1114e-04, -6.9170e-04, -1.0314e-03, -1.2743e-03, -1.0199e-03,\n",
       "         -1.2177e-03, -1.1182e-03, -1.0621e-03, -1.1234e-03, -1.4125e-03,\n",
       "         -1.2582e-03, -1.2213e-03, -8.7819e-04, -5.1930e-04, -9.4349e-05,\n",
       "          2.4515e-06,  4.3263e-05, -5.1975e-06, -7.0043e-05, -9.2222e-05,\n",
       "         -3.4216e-05,  6.1322e-05, -2.6001e-06,  5.4259e-05,  5.9735e-05,\n",
       "         -9.5766e-05, -1.0575e-05, -6.0718e-04, -8.1626e-04, -1.0540e-03,\n",
       "         -1.0286e-03, -1.0921e-03, -1.1048e-03, -1.2812e-03, -1.2480e-03,\n",
       "         -1.2234e-03, -1.1879e-03, -1.1562e-03, -1.1177e-03, -1.0214e-03,\n",
       "         -7.7408e-04, -6.9540e-04, -7.0628e-04, -3.9384e-04, -2.3424e-04,\n",
       "         -3.6842e-05,  2.5574e-05,  4.8981e-05,  4.8842e-05,  3.1729e-05,\n",
       "          7.3637e-05, -7.0394e-06,  8.9766e-05,  3.3997e-05, -5.6548e-04,\n",
       "         -7.8217e-04, -8.1254e-04, -1.0053e-03, -1.1741e-03, -1.0740e-03,\n",
       "         -1.0229e-03, -1.1482e-03, -9.9568e-04, -1.1931e-03, -1.2408e-03,\n",
       "         -1.1413e-03, -1.0324e-03, -9.8578e-04, -7.3760e-04, -7.7097e-04,\n",
       "         -1.0011e-03, -9.9178e-04, -4.8764e-04, -2.2673e-04, -2.1731e-04,\n",
       "         -8.7884e-05, -5.3177e-05, -8.2722e-05, -9.4236e-05, -8.0413e-05,\n",
       "          5.3672e-05, -3.4621e-05, -6.0473e-04, -7.4103e-04, -8.3731e-04,\n",
       "         -8.6209e-04, -1.0843e-03, -1.1020e-03, -1.1200e-03, -1.1599e-03,\n",
       "         -9.0465e-04, -1.0277e-03, -1.1634e-03, -1.2616e-03, -1.0969e-03,\n",
       "         -1.3050e-03, -1.1621e-03, -1.2290e-03, -1.2931e-03, -1.0334e-03,\n",
       "         -4.5686e-04, -1.7708e-04,  1.7494e-05, -2.7554e-05, -3.4154e-05,\n",
       "         -9.5406e-05,  7.9400e-05,  1.3201e-05, -1.4121e-05, -1.5365e-04,\n",
       "         -6.7313e-04, -7.1041e-04, -7.9163e-04, -8.8051e-04, -1.0264e-03,\n",
       "         -9.9795e-04, -9.8791e-04, -9.4053e-04, -7.7181e-04, -9.9126e-04,\n",
       "         -1.0796e-03, -1.2589e-03, -1.0528e-03, -1.1842e-03, -1.2109e-03,\n",
       "         -1.0120e-03, -8.6441e-04, -7.3169e-04, -6.7901e-04, -2.8661e-04,\n",
       "         -1.8457e-05, -8.7453e-05, -2.6899e-05, -3.3734e-06, -3.4874e-05,\n",
       "         -7.9093e-05,  4.3694e-05, -4.0105e-04, -7.2022e-04, -5.7866e-04,\n",
       "         -8.2527e-04, -8.7860e-04, -1.0826e-03, -8.5454e-04, -8.6083e-04,\n",
       "         -8.0300e-04, -8.1592e-04, -1.0896e-03, -9.8242e-04, -9.8728e-04,\n",
       "         -1.3007e-03, -1.1483e-03, -9.3001e-04, -1.0334e-03, -9.8959e-04,\n",
       "         -6.2652e-04, -6.6694e-04, -4.7371e-04, -2.5581e-04, -6.3555e-05,\n",
       "         -9.2438e-05,  5.0640e-05, -1.4550e-05, -1.7966e-05,  8.5239e-05,\n",
       "         -4.3716e-04, -4.4498e-04, -4.7394e-04, -5.9101e-04, -7.3414e-04,\n",
       "         -9.6481e-04, -6.7087e-04, -8.9589e-04, -1.1161e-03, -1.1393e-03,\n",
       "         -1.3429e-03, -1.3939e-03, -1.5368e-03, -1.3179e-03, -1.0790e-03,\n",
       "         -8.1242e-04, -8.6401e-04, -8.6170e-04, -7.5098e-04, -8.4974e-04,\n",
       "         -4.5787e-04, -3.2405e-04, -1.7093e-04, -6.8655e-05, -3.6568e-05,\n",
       "          1.4413e-06,  2.5494e-05, -2.1860e-05, -5.0779e-04, -6.4987e-04,\n",
       "         -4.7259e-04, -4.5011e-04, -6.7620e-04, -1.1054e-03, -8.3649e-04,\n",
       "         -9.0736e-04, -8.8168e-04, -1.0072e-03, -9.9017e-04, -9.8843e-04,\n",
       "         -9.4966e-04, -9.3019e-04, -1.2309e-03, -9.5924e-04, -6.9941e-04,\n",
       "         -6.1912e-04, -5.1798e-04, -5.6292e-04, -6.4715e-04, -4.0735e-04,\n",
       "         -2.5645e-04, -3.6218e-05,  5.4403e-06, -9.3340e-05,  4.7988e-05,\n",
       "         -1.9370e-04, -4.9568e-04, -5.6017e-04, -5.8942e-04, -6.1697e-04,\n",
       "         -6.2004e-04, -9.3143e-04, -7.4341e-04, -7.3458e-04, -8.4835e-04,\n",
       "         -1.0033e-03, -8.6906e-04, -9.3394e-04, -9.8426e-04, -9.7922e-04,\n",
       "         -1.1653e-03, -1.2236e-03, -8.2819e-04, -9.1690e-04, -8.1202e-04,\n",
       "         -8.2188e-04, -5.8603e-04, -4.0311e-04, -2.0910e-04, -8.7811e-05,\n",
       "         -9.2247e-05, -5.1780e-05, -6.2521e-05, -7.4103e-05, -6.1703e-04,\n",
       "         -5.6655e-04, -7.0161e-04, -6.8665e-04, -8.2227e-04, -1.1104e-03,\n",
       "         -1.2069e-03, -1.1860e-03, -1.1476e-03, -1.2775e-03, -1.1416e-03,\n",
       "         -1.3541e-03, -1.3254e-03, -1.4186e-03, -1.5170e-03, -1.3776e-03,\n",
       "         -1.1069e-03, -9.4141e-04, -6.7485e-04, -6.9172e-04, -6.7651e-04,\n",
       "         -4.1426e-04, -3.5371e-04, -1.5535e-04, -5.4844e-05,  6.4473e-05,\n",
       "          4.3885e-05, -1.2374e-05, -3.2546e-04, -3.4444e-04, -5.1360e-04,\n",
       "         -6.4235e-04, -6.6190e-04, -1.0205e-03, -1.0311e-03, -1.0061e-03,\n",
       "         -1.0085e-03, -1.2267e-03, -1.2628e-03, -1.3009e-03, -1.2730e-03,\n",
       "         -1.3013e-03, -1.1574e-03, -1.2616e-03, -1.1076e-03, -9.2347e-04,\n",
       "         -7.0105e-04, -7.9018e-04, -8.2888e-04, -6.4892e-04, -3.8401e-04,\n",
       "         -9.6633e-05,  9.4074e-05, -1.6272e-05, -9.6811e-06, -1.9717e-04,\n",
       "         -4.4128e-04, -3.5554e-04, -4.5923e-04, -4.4269e-04, -6.6186e-04,\n",
       "         -1.0654e-03, -1.1769e-03, -1.1961e-03, -1.2968e-03, -1.3431e-03,\n",
       "         -1.4166e-03, -1.4123e-03, -1.3552e-03, -1.3126e-03, -1.2221e-03,\n",
       "         -1.3525e-03, -9.4945e-04, -9.5562e-04, -9.2299e-04, -9.1002e-04,\n",
       "         -1.0394e-03, -4.1610e-04, -4.1622e-04, -9.1947e-05,  3.1510e-05,\n",
       "         -3.6572e-05, -9.1772e-05, -1.9497e-05, -4.0394e-04, -4.4303e-04,\n",
       "         -3.9335e-04, -4.2127e-04, -7.1521e-04, -1.2783e-03, -1.2676e-03,\n",
       "         -1.2587e-03, -1.1722e-03, -1.4056e-03, -1.4118e-03, -1.2822e-03,\n",
       "         -1.3042e-03, -1.3199e-03, -1.2951e-03, -1.0749e-03, -1.0918e-03,\n",
       "         -1.2845e-03, -8.0968e-04, -8.7480e-04, -6.4665e-04, -5.4349e-04,\n",
       "         -3.2347e-04, -1.0272e-05,  2.3339e-05,  8.8729e-05,  5.1057e-05,\n",
       "         -1.9164e-04, -5.1770e-04, -4.3344e-04, -6.0556e-04, -5.3197e-04,\n",
       "         -8.2874e-04, -1.4242e-03, -1.1528e-03, -1.3405e-03, -1.3561e-03,\n",
       "         -1.4586e-03, -1.4356e-03, -1.3752e-03, -1.3958e-03, -1.3717e-03,\n",
       "         -1.4118e-03, -1.3689e-03, -1.2215e-03, -9.3657e-04, -6.6903e-04,\n",
       "         -5.9294e-04, -9.0724e-04, -8.7265e-04, -3.8566e-04, -1.3001e-04,\n",
       "          4.0915e-05, -1.1540e-04, -5.6505e-05, -4.4075e-04, -5.3486e-04,\n",
       "         -5.8796e-04, -7.0433e-04, -4.9941e-04, -8.5514e-04, -1.2478e-03,\n",
       "         -1.2553e-03, -1.4022e-03, -1.4300e-03, -1.2915e-03, -1.4327e-03,\n",
       "         -1.3683e-03, -1.3722e-03, -1.3499e-03, -1.4869e-03, -1.4610e-03,\n",
       "         -8.6968e-04, -9.9594e-04, -8.7682e-04, -9.7321e-04, -8.6094e-04,\n",
       "         -6.9487e-04, -2.9926e-04, -6.4388e-05, -5.4357e-05, -2.5637e-04,\n",
       "         -2.1993e-04, -5.2826e-04, -5.8318e-04, -6.9949e-04, -6.2815e-04,\n",
       "         -4.8763e-04, -1.0779e-03, -1.3828e-03, -1.1800e-03, -1.2548e-03,\n",
       "         -1.1650e-03, -1.3828e-03, -1.3633e-03, -1.2486e-03, -1.2494e-03,\n",
       "         -1.6013e-03, -1.7625e-03, -1.4876e-03, -9.1123e-04, -8.5990e-04,\n",
       "         -6.9333e-04, -7.3921e-04, -8.6261e-04, -7.6450e-04, -3.5020e-04,\n",
       "         -8.4300e-05,  4.9055e-05, -1.2188e-04, -2.1695e-04, -5.9525e-04,\n",
       "         -7.7907e-04, -8.3383e-04, -6.8972e-04, -6.1022e-04, -1.3636e-03,\n",
       "         -1.3894e-03, -1.2441e-03, -1.3564e-03, -1.3877e-03, -1.3136e-03,\n",
       "         -1.4721e-03, -1.3737e-03, -1.5574e-03, -1.7271e-03, -1.5936e-03,\n",
       "         -1.4644e-03, -8.5719e-04, -7.6570e-04, -8.5592e-04, -7.0209e-04,\n",
       "         -8.5703e-04, -6.6686e-04, -3.4099e-04, -1.5266e-04,  6.3741e-05,\n",
       "         -1.1860e-04, -2.0906e-04, -3.7739e-04, -5.8663e-04, -7.6965e-04,\n",
       "         -9.1531e-04, -7.3391e-04, -1.3920e-03, -1.5356e-03, -1.1913e-03,\n",
       "         -1.3069e-03, -1.2124e-03, -1.2115e-03, -1.2003e-03, -1.3479e-03,\n",
       "         -1.6538e-03, -1.2704e-03, -1.2742e-03, -1.2668e-03, -8.5954e-04,\n",
       "         -5.4459e-04, -6.5124e-04, -6.1298e-04, -8.3055e-04, -5.6741e-04,\n",
       "         -4.5546e-05,  7.4643e-05, -9.5870e-05, -6.1432e-05, -6.3339e-05,\n",
       "         -4.7264e-04, -8.1272e-04, -8.4152e-04, -8.7284e-04, -7.2222e-04,\n",
       "         -1.3437e-03, -1.3723e-03, -1.2785e-03, -1.2218e-03, -1.1618e-03,\n",
       "         -1.2492e-03, -1.0655e-03, -1.3940e-03, -1.5282e-03, -1.3255e-03,\n",
       "         -1.2190e-03, -1.2003e-03, -9.1690e-04, -7.6517e-04, -5.5918e-04,\n",
       "         -6.8183e-04, -7.2381e-04, -4.6367e-04, -1.4133e-04, -1.0903e-04,\n",
       "         -1.1420e-04, -2.7724e-04, -2.0779e-04, -3.6737e-04, -7.2275e-04,\n",
       "         -7.6282e-04, -8.7837e-04, -7.5135e-04, -1.3852e-03, -1.3721e-03,\n",
       "         -1.3567e-03, -1.1917e-03, -1.0386e-03, -1.1637e-03, -1.4307e-03,\n",
       "         -1.5710e-03, -1.4659e-03, -1.2138e-03, -1.0981e-03, -1.1437e-03,\n",
       "         -7.9299e-04, -5.8938e-04, -6.5087e-04, -6.8728e-04, -7.6275e-04,\n",
       "         -4.6441e-04, -1.7484e-04,  9.8159e-06, -1.1001e-04, -4.4180e-04,\n",
       "         -5.3431e-04, -6.3754e-04, -7.7237e-04, -8.1048e-04, -7.6296e-04,\n",
       "         -5.9305e-04, -1.4168e-03, -1.3333e-03, -1.4150e-03, -1.2509e-03,\n",
       "         -1.2372e-03, -1.3248e-03, -1.6221e-03, -1.5899e-03, -1.2913e-03,\n",
       "         -1.1973e-03, -1.0912e-03, -1.1625e-03, -8.8577e-04, -5.0916e-04,\n",
       "         -6.1864e-04, -5.3938e-04, -7.2135e-04, -5.7229e-04, -6.1628e-05,\n",
       "         -1.2415e-04,  5.4906e-05, -2.7330e-04, -5.3799e-04, -5.3203e-04,\n",
       "         -6.3692e-04, -7.9154e-04, -7.1769e-04, -8.5951e-04, -1.5142e-03,\n",
       "         -1.5387e-03, -1.3356e-03, -1.5046e-03, -1.2938e-03, -1.5364e-03,\n",
       "         -1.7412e-03, -1.3019e-03, -1.1045e-03, -1.1295e-03, -1.1250e-03,\n",
       "         -1.2625e-03, -9.9367e-04, -5.1552e-04, -5.9676e-04, -5.4299e-04,\n",
       "         -5.4276e-04, -4.8622e-04, -1.1064e-04, -9.0881e-05,  1.6214e-05,\n",
       "         -1.8647e-04, -5.7327e-04, -5.4378e-04, -7.4229e-04, -7.7376e-04,\n",
       "         -7.3867e-04, -7.4304e-04, -1.3179e-03, -1.4203e-03, -1.1146e-03,\n",
       "         -1.1766e-03, -1.2006e-03, -1.1794e-03, -1.1674e-03, -9.4746e-04,\n",
       "         -1.0040e-03, -8.8351e-04, -8.6916e-04, -1.0302e-03, -7.6638e-04,\n",
       "         -3.8466e-04, -5.7229e-04, -6.3416e-04, -7.0228e-04, -5.0899e-04,\n",
       "         -2.6558e-04,  4.5505e-05, -8.2760e-05, -1.1336e-04, -4.2631e-04,\n",
       "         -6.1666e-04, -8.4582e-04, -8.7228e-04, -8.5950e-04, -6.7366e-04,\n",
       "         -1.3613e-03, -1.3279e-03, -1.3159e-03, -1.3109e-03, -1.2841e-03,\n",
       "         -1.0885e-03, -7.7014e-04, -8.0989e-04, -8.9408e-04, -8.8595e-04,\n",
       "         -7.2026e-04, -9.6165e-04, -6.6810e-04, -4.1746e-04, -5.9487e-04,\n",
       "         -7.3358e-04, -5.8836e-04, -4.7129e-04, -5.3486e-05,  5.3839e-06,\n",
       "          1.9151e-05, -3.6603e-05, -1.5780e-05, -2.0155e-04, -6.9642e-04,\n",
       "         -7.7858e-04, -6.5322e-04, -4.8469e-04, -9.3332e-04, -1.3130e-03,\n",
       "         -1.1934e-03, -1.2316e-03, -1.1079e-03, -7.6040e-04, -4.7923e-04,\n",
       "         -7.4889e-04, -7.1102e-04, -8.7698e-04, -7.8836e-04, -8.9033e-04,\n",
       "         -4.0800e-04, -2.6000e-04, -5.0233e-04, -6.8991e-04, -5.0620e-04,\n",
       "         -3.5680e-04, -2.9331e-05, -3.9857e-05, -5.8281e-05,  3.2824e-05,\n",
       "          5.4690e-06, -9.2153e-05, -4.9762e-04, -4.2193e-04, -5.3184e-04,\n",
       "         -2.6329e-04, -3.9810e-04, -7.8352e-04, -7.8115e-04, -8.3268e-04,\n",
       "         -8.8050e-04, -5.4135e-04, -4.4092e-04, -7.4535e-04, -7.2537e-04,\n",
       "         -6.6393e-04, -7.0888e-04, -4.6226e-04, -2.7669e-04, -1.7413e-04,\n",
       "         -3.7178e-04, -6.0299e-04, -3.5548e-04, -2.6205e-04, -6.2500e-05,\n",
       "          8.3497e-05,  5.1117e-05,  6.3120e-05, -5.6019e-05,  2.2924e-05,\n",
       "         -1.7314e-04, -2.8174e-04, -1.6975e-04, -6.4816e-05, -1.7796e-04,\n",
       "         -2.2432e-04, -1.9701e-04, -2.1560e-04, -2.5811e-04, -1.5523e-04,\n",
       "         -2.0797e-04, -3.5013e-04, -3.4996e-04, -1.9088e-04, -1.3505e-04,\n",
       "         -9.7111e-05, -8.6452e-05, -5.9567e-05, -2.0304e-04, -4.5160e-04,\n",
       "         -2.9959e-04, -8.6959e-06,  4.3683e-05,  4.6388e-05]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.conv2(nn.Conv2d(6, 16, 5))\n",
    "        self.fc1 = nn.Linear(120, 84)\n",
    "        self.fc2 = nn.Linear(84, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc1(c))\n",
    "        return x\n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        size = x[1:]\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s \n",
    "        return num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision \n",
    "import torchvision.transforms as transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data\\cifar-10-python.tar.gz\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                         shuffle=False, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
